{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Introduction\n",
    "In this lab we learn some terminology, gain some background of some basic concepts and learn how to use the Scikit-learn library to train different models to perform predictions.\n",
    "\n",
    "> To keep things simple we will keep the main focus on categorization using supervised learning\n",
    "\n",
    "## <a id=\"toc\"></a>Table of Contents\n",
    " * [Iris Data set](#iris_dataset)\n",
    " * [Observations and Features](#observation_features)\n",
    " * [Modeling](#modeling)\n",
    " * [Train/Test](#train_test)\n",
    " * [Model and Parameter Choices](#choices)\n",
    " * [Validation set - Hold-out set](#validation)\n",
    " * [Hyperparameter Selection](#selection)\n",
    "\n",
    "\n",
    "# <a id=\"iris_dataset\"></a>Iris Data set\n",
    "For this lab we will use the famous iris data set. This data set is probably the best known database to be found in pattern recognition literature. The data set contains 3 classes with 50 instances each, and each class refers to a type of iris plant. This data set is generally included in most of the libraries as a sample. However you could download it from https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "> __Tip:__ This link, like many others on the internet, includes a number of other samples that you can use for learning\n",
    "\n",
    "Our first step is to import the data set, as included within the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Iris Data Set\n",
    "In this step we will load and store the iris data set into a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine `iris` data type\n",
    "Now that we have loaded the iris data set in variable called `iris`, let's see what type it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable details\n",
    "We can see that it is of type `sklearn.utils.Bunch`\n",
    "\n",
    "Our expectation would have been to see a list or maybe a list of lists. So let's examine the type and see what this type offers.\n",
    "\n",
    "__Q:__ _But how do we know what attributes or functions are offered by `iris`?_\n",
    "\n",
    "The simple answer is to use the online scikit-learn documentation (Bunch objects are just a way to package some numpy arrays) [more info](http://scikit-learn.org/stable/faq.html#how-can-i-create-a-bunch-object), but there is a quicker way to get an initial idea.\n",
    "\n",
    "> __Tip:__ use the completion feature in Jupyter notebooks by pressing the `tab` key combination\n",
    "\n",
    "Let's examine the attributes and function offered by iris variable using the completion feature. This helps us learn that we have `iris.feature_names`, `iris.data`, `iris.target_names` and `iris.target`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `iris`\n",
    "Let's further look into the details of the values we find in the `iris`\n",
    "\n",
    "Let's check the lengths of the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q:__ _Think about what are all these values good for and why are they different sizes?_\n",
    "\n",
    "> __Tip:__ you can use the slice notation `[a:b]` to see slices of the values in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "One of the data types commonly used is numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above the `iris.data` attribute is of type `numpy.ndarray`, which offers many benefits over simple lists available in python, and they are used quite frequently\n",
    "\n",
    "A good place to learn about _numpy_ is https://docs.scipy.org/doc/numpy-1.13.0/user/quickstart.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"observation_features\"></a>Observations and Features\n",
    "\n",
    "Now that we have examined our dataset a little bit, let's dig into some terminology\n",
    "\n",
    "- __Observations__ are the individual data samples, or records\n",
    "- __Features__ are the each of the attributes in the data that are specifically selected, perhaps from a large set of possible attributes that are important to our analysis and modeling\n",
    "\n",
    "So in other words, if you imagine your data as a table, the rows in the table are the _observations_, while the columns are the _features_.\n",
    "\n",
    "## Observations and Features in Iris data set\n",
    "How do we determine what the _iris_ data set looks like?\n",
    "\n",
    "Recall from examining the iris dataset (i.e. `iris`) that `iris.feature_names` has a length of _4_, and `irsi.data` has a length of 150\n",
    "\n",
    "This tells us that we have 150 __observations__ and 4 __features__\n",
    "\n",
    "You might ask how can we infer this from the data itself? We could use the `shape` attribute of the `iris.data` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this tells us that we have 150 rows (observations) with 4 columns (features) per row\n",
    "\n",
    "> __Note:__ the fact that we have 4 features may not mean that all of them are always important, you might have data that has 10 features, and during your analysis and modeling, you would use certain combinations of features to better gain insight\n",
    "\n",
    "## Labeled training data\n",
    "When talking about a categorization problem in the context of _supervised learning_, we need to have _labeled data_\n",
    "\n",
    "In this `iris` data set the labeled data is available in the _target_ attribute You can think of each row of the data in the `iris.target` attribute as the response to the categorization question of each observation in the `iris.data`. Put differently, the correct response to the question of _which category does `iris.data[0]` belong to?_ is `iris.target[0]`\n",
    "\n",
    "> __Q:__ _From the above explanation, and knowing the shape of the input (i.e. `iris.data`) that we saw earlier, what would you expect the shape of the `iris.target` to be?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would expect that the number of observations in the `iris.data` to match the number of rows of data in the `iris.target` attribute.\n",
    "\n",
    "### Iris target - Numeric\n",
    "You can also notice that all the values in `iris.target` attribute are numeric. This is because all the analysis and modeling is based on mathematical calculations and the results have to be numeric. However to make things more meaningful for representation, the `iris.target_name` has the textual representation of the numbers (e.g. 0, 1, 2 in  this case)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([iris.target_names[i] for i in iris.target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a id=\"modeling\"></a>Modeling\n",
    "Now that we know what our data looks like, let's use scikit-learn to train a model and make predictions.\n",
    "\n",
    "The nice thing about scikit-learn is that it follows a simple set of steps in a consistent fashion across any of the available models. This makes learning somewhat easier, and you can use the learning from one library and get started on other libraries much faster.\n",
    "\n",
    "__First some conventions__\n",
    "- Upper case is used to represent a list of lists\n",
    "- Lower case is for simple lists (e.g. vector)\n",
    "\n",
    "Let's use X to signify the input and y the response.\n",
    "\n",
    "> __Q:__ _What do you think the X and y should be for our iris example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use a model, train it, and try a simple prediction. We are doing this just so that we learn the pattern.\n",
    "\n",
    "For this example we will use the __K-Nearest Neighbors__ classifier algorithm, so let's first import it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are quite a few arguments available when trying to instantiate this class, but luckily scikit-learn has some sensible defaults to help you get started faster\n",
    "\n",
    "In this case we will use `n_neighbors=1` rather than the default value which is 5\n",
    "\n",
    "> __Q:__ *How do we know the default is 5?* (_hint:_ content assist)\n",
    "\n",
    "By the way, these tuning parameters are sometimes also referred to as __hyper parameters__, and they make a considerable impact on the model's behavior and prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so now that we have an instance of this classifier, let's use it to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our model instance is trained with all of the iris dataset\n",
    "\n",
    "Let's try doing a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.data[5], iris.target[5])\n",
    "print(iris.data[100], iris.target[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = [[5.4,  3.9,  1.7,  0.4]]\n",
    "knn.predict(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Q:__ *why does the X1 test data above have two brackets?*\n",
    "  _hint:_ remember the shape of our input data\n",
    "\n",
    "Let's try another test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = [[5.4,  3.9,  1.7,  0.4], [6.3,  3.3,  6. ,  2.5]]\n",
    "knn.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same steps with a different parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X, y)\n",
    "knn.predict(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we decided to use a different model?\n",
    "\n",
    "Let's use a __Logistic Regression__ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "logreg.predict(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Note:__ If we had an accuracy of 100% it doesn't mean that our model is perfect, it most likely means we have __overfit__ our model, and it will do a great job of predicting values it has already been given, but will not do well on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"train_test\"></a>Train/Test\n",
    "\n",
    "So far we have explored how to work with Scikit-learn and a couple of its modeling algorithms.\n",
    "\n",
    "Let's look at a better way of training our model. Training a model requires large amounts of data, however validating that the model worked also needs a reasonable set. The general practice is to split the labeled data into a _Training_ and a _Test_ set. You would use the training set to train the model, and then use the test set (which is distinct and separate from the training set) to see how well the trained model is doing.\n",
    "\n",
    "One simple way to achieve this is to take the entire data set, and set aside 30% of the entries with a random selection. So the model is trained with 70% of the data, and then its tested with the remaining 30% test set, which the model hasn't seen before.\n",
    "\n",
    "It might be easy to write a couple of lines of code to do this, but you would be doing this every time. Scikit-learn provides a simple way to randomly split your data set into training and test set.\n",
    "\n",
    "Let's use this approach and perform a training and then test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above snippet, we imported `train_test_split`, and used it to split the iris data set into two pairs of training and test set data. We will use the `X_train` training set with its corresponding `y_train` response, and when we are ready to test our model (after training), we will use the `X_test` test data set with its corresponding response data `y_test`\n",
    "\n",
    "Let's briefly explore the shapes of the data we just split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Note:__ Why 97 and 53? 150 * .35 = 52.5 or rounded to 53\n",
    "\n",
    "Now let's use the data and perform the training and testing activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained the model with the training data, let's see the prediction results from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the above predictions are generated by the model, and we will have to compare it with the correct values from our data set as stored in the `y_test` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are achieving a 94% accuracy\n",
    "\n",
    "Let's try this with KNN (like we did earlier) with a K=5 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we achieved an accuracy of 98%\n",
    "\n",
    "> __Q:__ *How can we determine the most suitable K value*\n",
    "\n",
    "A simple way that comes to mind is to repeat the above steps in a loop and change the K value, and then check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(1, 26):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(\"K=%d\\t=> %f\" %(k, metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the values above we can see that __K=12__ has includes the best results while including larger number of neighbors. What we need to do next is to use this value of K, and train our model with the __entire training data__ that we have (i.e. `X` and _not_ just `X_train`), and then start using it with unseen data and gauge its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=12)\n",
    "knn.fit(X, y)\n",
    "knn.predict(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# <a id=\"choices\"></a>Model and Parameter Choices\n",
    "So far we looked at two ways of training our data\n",
    "- Using the entire data set for training and then testing with the same data\n",
    "- Splitting the data into a training set and a testing set\n",
    "\n",
    "In the first case we have no way of knowing how well each model and its various parameters would do outside of our own known data set. So this is not effective\n",
    "However in the second case, we at least get some idea how well the model would be doing because we have a testing set, which would help us in model and parameter decision making.\n",
    "\n",
    "The second approach is a step in the right direction. Imagine if we had a way to generate multiple splits of the data set into training and testing data sets to give us the advantage of training the data with a variety of record mixes; and averaged out the accuracy of the model across these variations. It also enables a more efficient use of the data set. This is essentially what __K Fold Cross Validation__ offers!\n",
    "\n",
    "> Note: this is a more time consuming method compared to train/test split, because you would be running the training sessions K times compared to single time with train/test split.\n",
    "\n",
    "## Cross Validation\n",
    "The following image shows how this works\n",
    "![cross validation](images/CrossValidation.png)\n",
    "\n",
    "In Scikit-learn this is offered by the `cross_val_score` class. In this example we will use `K=5` for the _KNN_ model and perform a 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the average score as an indicator of how well our model will do when run on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a more effective (albeit slower) way of using our data, we can also find the best `K` value for our _KNN_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kRange = range(1, 31)\n",
    "scoresByK = []\n",
    "for k in kRange:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    scoresByK.append((scores.mean()))\n",
    "    print(\"K=%d\\t=> %f\" %(k, scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these values to see a visual representation of this data\n",
    "\n",
    "The X-axis represents the `K` values for _KNN_\n",
    "\n",
    "The Y-axis represents the _Cross Validation Accuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(kRange, scoresByK)\n",
    "plt.xlabel('K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the _KNN_ and _Logistic Regression_ model results using _cross validation_ and find the best between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=20)\n",
    "print(\"KNN accuracy\\t= %f\" %cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "print(\"LogReg accuracy\\t= %f\" %cross_val_score(logreg, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"validation\"></a>Validation set - Hold-out set\n",
    "You can set some data aside in the beginning to use at the end of train/test or cross-validation training. This data is truly out of sample and can be a good indicator of how well the model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"selection\"></a>Hyperparameter Selection\n",
    "So far we compared the performance of single models, or explored changing a single hyperparameter value. It appears that to get the best combination of _hyperparameters_ for, any model, would mean writing some code. The code would iterate over a permutation of different options. Let's say param_1 can be _1_ or _2_, and param_2 can be _a_ or _b_, then you would have to write code to test the model with these combinations of param_1 and param_2 respectively\n",
    "- _1 & a_\n",
    "- _1 & b_\n",
    "- _2 & a_\n",
    "- _2 & b_\n",
    "\n",
    "And imagine changing that code every time you need to add more parameters or change them!\n",
    "\n",
    "Scikit-learn offers a simple solution to this: __GridSearchCV__\n",
    "\n",
    "`GridSearchCV` simply allows you to define the parameters and their possible values and then apply them to a model in a single line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "kRange = range(1, 30)\n",
    "param_grid = dict(n_neighbors=kRange)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X, y)\n",
    "gridMeanScores = grid.cv_results_[\"mean_test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kRange, gridMeanScores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Best Score = \", grid.best_score_\n",
    "print \"Best Params = \", grid.best_params_\n",
    "print \"Best Estimator = \", grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example used a single parameter, but the same technique can be used to add more options to the parameters. For example we could define `param_grid = dict(n_neighbors=kRange, weights=weightOptions)` where `weightOptions = ['uniform', 'distance']`. This would cause a larger set of tests where each `K` value is paired once with _uniform_ and the _distance_ for the _weights_ parameter.\n",
    "\n",
    "This method allows a simpler way of working with multiple permutation and combinations of parameter usage.\n",
    "\n",
    "As you can imagine, a larger number of variations could cause a huge performance concern. The __RandomizedSearchCV__ class offers some relief by randomly selecting combinations, and as you can imagine the tradeoff of faster computation is less accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
